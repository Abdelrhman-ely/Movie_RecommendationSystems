# ðŸŽ¬ Movie Recommendation System

A full end-to-end machine learning project that builds a **personalized movie recommendation system** using a two-stage **Retrieval + Ranking** architecture, trained on the MovieLens 1M dataset, and deployed with **FastAPI** + **Streamlit**.

---

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Pipeline](#pipeline)
- [Model Architecture](#model-architecture)
- [Results](#results)
- [Installation](#installation)
- [Running the App](#running-the-app)
- [API Reference](#api-reference)
- [Tech Stack](#tech-stack)

---

## Overview

This project implements a production-style recommendation system inspired by how large platforms like YouTube and Netflix serve personalized content. The system works in two stages:

1. **Retrieval** â€” quickly finds ~200 candidate movies for a user using a Two-Tower neural network and cosine similarity
2. **Ranking** â€” re-ranks the candidates using a deep neural network that predicts the exact rating a user would give each movie

---

## Dataset

**MovieLens 1M** â€” a classic benchmark dataset for recommendation systems.

| File | Description | Size |
|------|-------------|------|
| `ratings.dat` | user_id Â· movie_id Â· rating (1â€“5) Â· timestamp | 1,000,209 ratings |
| `movies.dat` | movie_id Â· title Â· genres | 3,952 movies |
| `users.dat` | user_id Â· gender Â· age Â· occupation Â· zip | 6,040 users |

**Key stats:**
- Rating range: 1 â€“ 5 stars
- 56.8% of ratings are 4 or 5 stars (positive skew)
- 18 unique genres (pipe-separated per movie)
- Data split: **80% train / 20% test** (chronological â€” sorted by timestamp)

---

## Project Structure

```
recommendationSysetms/
â”œâ”€â”€ main.py                        â† FastAPI backend
â”œâ”€â”€ streamlit_app.py               â† Streamlit frontend
â”œâ”€â”€ recommendation_system.ipynb    â† Training notebook
â”œâ”€â”€ requirements.txt               â† Python dependencies
â”œâ”€â”€ models/                        â† Saved artifacts
â”‚   â”œâ”€â”€ user_tower.keras
â”‚   â”œâ”€â”€ movie_tower.keras
â”‚   â”œâ”€â”€ ranking_model.keras
â”‚   â”œâ”€â”€ retrieval_model.keras
â”‚   â”œâ”€â”€ all_movie_vectors.npy
â”‚   â”œâ”€â”€ encoders.pkl
â”‚   â”œâ”€â”€ year_scaler.pkl
â”‚   â”œâ”€â”€ genre_cols.pkl
â”‚   â”œâ”€â”€ movie_full.parquet
â”‚   â””â”€â”€ users.parquet
â””â”€â”€ ml-1m/                         â† Raw dataset
    â”œâ”€â”€ ratings.dat
    â”œâ”€â”€ movies.dat
    â””â”€â”€ users.dat
```

---

## Pipeline

```
Raw Data
   â”‚
   â–¼
Exploratory Data Analysis (EDA)
   â”‚  â€¢ Rating distribution
   â”‚  â€¢ Top rated / most rated movies
   â”‚  â€¢ User demographics
   â–¼
Preprocessing & Feature Engineering
   â”‚  â€¢ Sequential ID encoding (users & movies)
   â”‚  â€¢ Genre one-hot encoding (18 genres)
   â”‚  â€¢ Release year extraction + MinMax scaling
   â”‚  â€¢ User features: gender, age group, occupation
   â”‚  â€¢ Chronological train/test split (80/20)
   â–¼
Stage 1 â€” Retrieval Model (Two-Tower)
   â”‚  â€¢ User Tower: embeddings â†’ Dense(64) â†’ L2 normalize
   â”‚  â€¢ Movie Tower: embeddings + genre + year â†’ Dense(64) â†’ L2 normalize
   â”‚  â€¢ Dot product â†’ Sigmoid â†’ Binary cross-entropy loss
   â”‚  â€¢ Output: Top-200 candidate movies per user
   â–¼
Stage 2 â€” Ranking Model
   â”‚  â€¢ Input: [user_vector(64) â€– movie_vector(64)] = 128 features
   â”‚  â€¢ Dense(256) â†’ BN â†’ Dropout â†’ Dense(128) â†’ BN â†’ Dropout â†’ Dense(64) â†’ Linear
   â”‚  â€¢ MSE loss â†’ predicts exact rating (1â€“5)
   â–¼
Saved Artifacts
   â”‚  â€¢ Models (.keras), encoders (.pkl), vectors (.npy), dataframes (.parquet)
   â–¼
Deployment
   â€¢ FastAPI  â†’ REST API (port 8000)
   â€¢ Streamlit â†’ Web UI  (port 8501)
```

---

## Model Architecture

### Retrieval Model â€” Two-Tower Network

```
User Tower                          Movie Tower
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Embedding(user_id,   32)            Embedding(movie_id, 32)
Embedding(gender,     8)            Genre OHE (18 features)
Embedding(age,        8)            Year scaled (1 feature)
Embedding(occupation, 8)                    â”‚
        â”‚                                   â”‚
   Concatenate                         Concatenate
        â”‚                                   â”‚
   Dense(64, ReLU)                    Dense(64, ReLU)
   Dropout(0.2)                       Dropout(0.2)
   Dense(64, ReLU)                    Dense(64, ReLU)
        â”‚                                   â”‚
  UnitNormalization               UnitNormalization
        â”‚                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€ Dot Product â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                  Dense(1, Sigmoid)
                       â”‚
               Binary Cross-Entropy
```

### Ranking Model â€” Deep Neural Network

```
Input: [user_vector(64) â€– movie_vector(64)] â†’ 128 features
         â”‚
   Dense(256, ReLU)
   BatchNormalization
   Dropout(0.3)
         â”‚
   Dense(128, ReLU)
   BatchNormalization
   Dropout(0.2)
         â”‚
   Dense(64, ReLU)
   Dropout(0.1)
         â”‚
   Dense(1, Linear)   â† predicted rating
         â”‚
       MSE Loss
```

---

## Results

| Model | Metric | Value |
|-------|--------|-------|
| Retrieval | AUC | > 0.75 |
| Ranking | RMSE | ~0.85 |
| Ranking | MAE | ~0.67 |

**Training config:**
- Optimizer: Adam (lr = 1e-3)
- Batch size: 2,048
- Early stopping: patience = 3 (monitoring val metric)
- ReduceLROnPlateau: factor = 0.5, patience = 2 (ranking only)

---

## Installation

```bash
# 1. Clone or download the project
cd recommendationSysetms

# 2. Create virtual environment
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # Mac/Linux

# 3. Install dependencies
pip install -r requirements.txt
```

---

## Running the App

You need **two terminals** open simultaneously.

### Terminal 1 â€” FastAPI Backend
```bash
uvicorn main:app --reload --port 8000
```
- API base: http://localhost:8000
- Interactive docs: http://localhost:8000/docs

### Terminal 2 â€” Streamlit Frontend
```bash
streamlit run streamlit_app.py
```
- Web UI: http://localhost:8501

---

## API Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Health check |
| `GET` | `/stats` | System statistics |
| `GET` | `/user/{user_id}` | User profile |
| `GET` | `/movie/{movie_id}` | Movie details |
| `POST` | `/recommend` | Get recommendations |

### Example â€” POST `/recommend`

**Request:**
```json
{
  "user_id": 1,
  "top_k_retrieve": 200,
  "top_n_final": 10
}
```

**Response:**
```json
{
  "user_id": 1,
  "gender": "F",
  "age": 18,
  "occupation": 10,
  "recommendations": [
    {
      "rank": 1,
      "movie_id": 1265,
      "title": "Groundhog Day (1993)",
      "genres": "Comedy|Romance",
      "year": 1993,
      "ranking_score": 4.52,
      "retrieval_score": 0.9823
    }
  ]
}
```

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Language | Python 3.12 |
| Deep Learning | TensorFlow 2.20 / Keras 3.13 |
| Data Processing | Pandas, NumPy, Scikit-learn |
| Backend API | FastAPI + Uvicorn |
| Frontend UI | Streamlit |
| Serialization | Pickle, Joblib, Parquet |
| Environment | Virtual Environment (venv) |

---

## Notes

- User IDs range from **1 to 6,040**
- First API startup takes ~30 seconds (models loading into memory)
- Movie vectors are **pre-computed** at startup for fast O(N) inference
- The `Lambda` layer in original models was replaced with `UnitNormalization` for Keras 3 serialization compatibility
